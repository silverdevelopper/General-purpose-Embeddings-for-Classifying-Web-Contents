{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "      <th>url</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>300201</th>\n",
       "      <td>Sommer and Behles A 19th-century Italian photo...</td>\n",
       "      <td>Engineering&amp;Technology</td>\n",
       "      <td>https://en.wikipedia.org/wiki?curid=8874959</td>\n",
       "      <td>Sommer and Behles</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>869203</th>\n",
       "      <td>Hague Convention for the Protection of Cultura...</td>\n",
       "      <td>Humanities</td>\n",
       "      <td>https://en.wikipedia.org/wiki?curid=2578344</td>\n",
       "      <td>Hague Convention for the Protection of Cultu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>523623</th>\n",
       "      <td>Email spam The amount of spam that users see i...</td>\n",
       "      <td>Social_sciences</td>\n",
       "      <td>https://en.wikipedia.org/wiki?curid=459847</td>\n",
       "      <td>Email spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>616638</th>\n",
       "      <td>Direct debit The payer can cancel the authoriz...</td>\n",
       "      <td>Business&amp;Economics</td>\n",
       "      <td>https://en.wikipedia.org/wiki?curid=2600081</td>\n",
       "      <td>Direct debit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186274</th>\n",
       "      <td>TCP-seq However, it captures positions of only...</td>\n",
       "      <td>Biology</td>\n",
       "      <td>https://en.wikipedia.org/wiki?curid=51165385</td>\n",
       "      <td>TCP-seq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1106115</th>\n",
       "      <td>Dambana Most of the remaining \"dambanas\" are n...</td>\n",
       "      <td>Religion&amp;Philosophy&amp;Ethics</td>\n",
       "      <td>https://en.wikipedia.org/wiki?curid=26211048</td>\n",
       "      <td>Dambana</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>493717</th>\n",
       "      <td>Memory Several genes, proteins and enzymes hav...</td>\n",
       "      <td>Social_sciences</td>\n",
       "      <td>https://en.wikipedia.org/wiki?curid=31217535</td>\n",
       "      <td>Memory</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>437418</th>\n",
       "      <td>Tilt-A-Whirl is a flat ride similar to the Wal...</td>\n",
       "      <td>Mechanical_engineering</td>\n",
       "      <td>https://en.wikipedia.org/wiki?curid=1612404</td>\n",
       "      <td>Tilt-A-Whirl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>779043</th>\n",
       "      <td>Judicial system in the United Arab Emirates Th...</td>\n",
       "      <td>Humanities</td>\n",
       "      <td>https://en.wikipedia.org/wiki?curid=49670559</td>\n",
       "      <td>Judicial system in the United Arab Emirates</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1170024</th>\n",
       "      <td>Global catastrophic risk It has been suggested...</td>\n",
       "      <td>Religion&amp;Philosophy&amp;Ethics</td>\n",
       "      <td>https://en.wikipedia.org/wiki?curid=21221594</td>\n",
       "      <td>Global catastrophic risk</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      text  \\\n",
       "300201   Sommer and Behles A 19th-century Italian photo...   \n",
       "869203   Hague Convention for the Protection of Cultura...   \n",
       "523623   Email spam The amount of spam that users see i...   \n",
       "616638   Direct debit The payer can cancel the authoriz...   \n",
       "186274   TCP-seq However, it captures positions of only...   \n",
       "...                                                    ...   \n",
       "1106115  Dambana Most of the remaining \"dambanas\" are n...   \n",
       "493717   Memory Several genes, proteins and enzymes hav...   \n",
       "437418   Tilt-A-Whirl is a flat ride similar to the Wal...   \n",
       "779043   Judicial system in the United Arab Emirates Th...   \n",
       "1170024  Global catastrophic risk It has been suggested...   \n",
       "\n",
       "                           category  \\\n",
       "300201       Engineering&Technology   \n",
       "869203                   Humanities   \n",
       "523623              Social_sciences   \n",
       "616638           Business&Economics   \n",
       "186274                      Biology   \n",
       "...                             ...   \n",
       "1106115  Religion&Philosophy&Ethics   \n",
       "493717              Social_sciences   \n",
       "437418       Mechanical_engineering   \n",
       "779043                   Humanities   \n",
       "1170024  Religion&Philosophy&Ethics   \n",
       "\n",
       "                                                  url  \\\n",
       "300201    https://en.wikipedia.org/wiki?curid=8874959   \n",
       "869203    https://en.wikipedia.org/wiki?curid=2578344   \n",
       "523623     https://en.wikipedia.org/wiki?curid=459847   \n",
       "616638    https://en.wikipedia.org/wiki?curid=2600081   \n",
       "186274   https://en.wikipedia.org/wiki?curid=51165385   \n",
       "...                                               ...   \n",
       "1106115  https://en.wikipedia.org/wiki?curid=26211048   \n",
       "493717   https://en.wikipedia.org/wiki?curid=31217535   \n",
       "437418    https://en.wikipedia.org/wiki?curid=1612404   \n",
       "779043   https://en.wikipedia.org/wiki?curid=49670559   \n",
       "1170024  https://en.wikipedia.org/wiki?curid=21221594   \n",
       "\n",
       "                                                     title  \n",
       "300201                                   Sommer and Behles  \n",
       "869203     Hague Convention for the Protection of Cultu...  \n",
       "523623                                          Email spam  \n",
       "616638                                        Direct debit  \n",
       "186274                                             TCP-seq  \n",
       "...                                                    ...  \n",
       "1106115                                            Dambana  \n",
       "493717                                              Memory  \n",
       "437418                                        Tilt-A-Whirl  \n",
       "779043         Judicial system in the United Arab Emirates  \n",
       "1170024                           Global catastrophic risk  \n",
       "\n",
       "[1000 rows x 4 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import joblib\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "import scipy.sparse as sp\n",
    "ds = pd.read_csv(\"../data/processed/wikipedia_m.csv\")\n",
    "df_big = pd.read_csv(\"../data/raw/wikipedia_science_chunked_small_rag_512.csv\")\n",
    "df_small = df_big.sample(n=32000, random_state=42122)\n",
    "df_small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_small = df_big.sample(n=32000, random_state=42122)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Load stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    # Remove numeric characters\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    # Tokenize\n",
    "    tokens = text.split()\n",
    "    # Remove stopwords\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    # Lemmatize\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    # Return the preprocessed text as a single string\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Assuming you have a DataFrame called df with columns 'text' and 'label'\n",
    "# Example:\n",
    "# df = pd.DataFrame({\n",
    "#     'text': ['This is text one', 'This is text two', 'Another text', ...],\n",
    "#     'label': [0, 1, 0, ...]\n",
    "# })\n",
    "# Split the data into training and testing sets\n",
    "ds_small['text'] = ds_small['text'].apply(preprocess_text)\n",
    "X = ds_small['text']\n",
    "ds_small['category_encoded'] = ds_small['category'].astype('category').cat.codes\n",
    "y = ds_small['category_encoded']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "def build_model(X_train:pd.core.series.Series):\n",
    "\n",
    "    # Prepare data for Doc2Vec\n",
    "    # `TaggedDocument` is needed for training the Doc2Vec model.\n",
    "    tagged_data = [TaggedDocument(words=text.split(), tags=[str(i)]) for i, text in enumerate(X_train)]\n",
    "\n",
    "    # Train Doc2Vec Model\n",
    "    doc2vec_model = Doc2Vec(vector_size=512,  # Dimensionality of the feature vectors\n",
    "                            window=4,       # The maximum distance between the current and predicted word\n",
    "                            min_count=2,    # Ignores words with a total frequency lower than this\n",
    "                            workers=8,      # Number of worker threads to use\n",
    "                            epochs=40)      # Number of training iterations\n",
    "\n",
    "    doc2vec_model.build_vocab(tagged_data)  # Build vocabulary\n",
    "    doc2vec_model.train(tagged_data, total_examples=len(tagged_data), epochs=doc2vec_model.epochs)  # Train model\n",
    "    return doc2vec_model\n",
    "\n",
    "# Transform the data into vectors\n",
    "# Use the trained Doc2Vec model to convert the text data into vector representations.\n",
    "def infer_vectors(doc2vec_model:Doc2Vec, X:pd.core.series.Series):\n",
    "    vector = [doc2vec_model.infer_vector(text.split()) for text in X]\n",
    "    return vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    4.1s\n",
      "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:   20.8s\n",
      "[Parallel(n_jobs=8)]: Done 300 out of 300 | elapsed:   34.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done 300 out of 300 | elapsed:    0.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.2721875\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.36      0.13      0.19       301\n",
      "           1       0.30      0.12      0.17       336\n",
      "           2       0.35      0.50      0.42       454\n",
      "           3       0.44      0.28      0.35       334\n",
      "           4       0.16      0.04      0.06       209\n",
      "           5       0.31      0.16      0.21       281\n",
      "           6       0.24      0.38      0.29       567\n",
      "           7       0.21      0.18      0.19       447\n",
      "           8       0.15      0.06      0.08       595\n",
      "           9       0.39      0.27      0.32       408\n",
      "          10       0.33      0.54      0.41       554\n",
      "          11       0.39      0.12      0.18       156\n",
      "          12       0.00      0.00      0.00        90\n",
      "          13       0.27      0.02      0.03       220\n",
      "          14       0.27      0.47      0.35       758\n",
      "          15       0.16      0.23      0.19       690\n",
      "\n",
      "    accuracy                           0.27      6400\n",
      "   macro avg       0.27      0.22      0.22      6400\n",
      "weighted avg       0.27      0.27      0.25      6400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train a classifier\n",
    "# Here, we use RandomForestClassifier but you can choose any classifier (e.g., SVM, Logistic Regression, etc.)\n",
    "doc2vec_model = build_model(X_train)\n",
    "X_train_vectors, X_test_vectors = infer_vectors(doc2vec_model, X_train), infer_vectors(doc2vec_model, X_test)\n",
    "clf = RandomForestClassifier(n_estimators=300,random_state=42,verbose=1,n_jobs=8)\n",
    "clf.fit(X_train_vectors, y_train)\n",
    "\n",
    "# Predict and evaluate the model on the test set\n",
    "y_pred = clf.predict(X_test_vectors)\n",
    "\n",
    "# Compute accuracy and classification report\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and train the XGBoost model\n",
    "def train_model(X_train, X_test, y_train, y_test):\n",
    "    xgb_model = XGBClassifier(use_label_encoder=False, \n",
    "                              eval_metric='mlogloss', \n",
    "                              objective='multi:softmax', \n",
    "                              num_class= 16,\n",
    "                              learning_rate=0.1, \n",
    "                              #max_depth=5, \n",
    "                              n_estimators=20,\n",
    "                              gemma=1\n",
    "                              )\n",
    "\n",
    "    # Train the XGBoost model with early stopping on the validation set\n",
    "    xgb_model.fit(X_train, y_train, eval_set=[(X_test, y_test)], verbose=True)\n",
    "    return xgb_model\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def evaluate(_X_test:np.array, _y_test:list, xgb_model):\n",
    "    _y_pred = xgb_model.predict(_X_test)\n",
    "    accuracy = accuracy_score(_y_test, _y_pred)\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    print(\"Classification Report:\\n\",classification_report(_y_test, _y_pred)) \n",
    "    return accuracy\n",
    "\n",
    "# Extract epoch-wise metrics\n",
    "def visualize_model(xgb_model):\n",
    "    from matplotlib import pyplot as plt\n",
    "    results = xgb_model.evals_result_  # Get the evaluation results during training\n",
    "\n",
    "    # Print results keys\n",
    "    print(\"Keys in evals_result_:\", results.keys())\n",
    "    # Accessing specific metrics\n",
    "    validation_loss = results['validation_0']['mlogloss']  # Epoch-wise loss on validation set\n",
    "    epochs = range(1, len(validation_loss) + 1)  # Epoch numbers\n",
    "    # Plot the loss over epochs\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(epochs, validation_loss, label='Validation Loss', marker='o')\n",
    "    plt.title('Validation Loss Across Epochs')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Log Loss')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ngumus/projects/Master-Thesis/.venv/lib/python3.10/site-packages/xgboost/core.py:158: UserWarning: [00:33:06] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"gemma\", \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-mlogloss:2.71583\n",
      "[1]\tvalidation_0-mlogloss:2.66875\n",
      "[2]\tvalidation_0-mlogloss:2.62810\n",
      "[3]\tvalidation_0-mlogloss:2.59179\n",
      "[4]\tvalidation_0-mlogloss:2.56201\n",
      "[5]\tvalidation_0-mlogloss:2.53393\n",
      "[6]\tvalidation_0-mlogloss:2.50653\n",
      "[7]\tvalidation_0-mlogloss:2.48187\n",
      "[8]\tvalidation_0-mlogloss:2.45986\n",
      "[9]\tvalidation_0-mlogloss:2.43875\n",
      "[10]\tvalidation_0-mlogloss:2.41934\n",
      "[11]\tvalidation_0-mlogloss:2.40094\n",
      "[12]\tvalidation_0-mlogloss:2.38423\n",
      "[13]\tvalidation_0-mlogloss:2.36868\n",
      "[14]\tvalidation_0-mlogloss:2.35326\n",
      "[15]\tvalidation_0-mlogloss:2.33820\n",
      "[16]\tvalidation_0-mlogloss:2.32558\n",
      "[17]\tvalidation_0-mlogloss:2.31211\n",
      "[18]\tvalidation_0-mlogloss:2.29966\n",
      "[19]\tvalidation_0-mlogloss:2.28672\n",
      "Accuracy: 0.28796875\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.37      0.26      0.31       301\n",
      "           1       0.30      0.23      0.26       336\n",
      "           2       0.43      0.46      0.44       454\n",
      "           3       0.36      0.23      0.28       334\n",
      "           4       0.30      0.22      0.25       209\n",
      "           5       0.32      0.23      0.27       281\n",
      "           6       0.22      0.27      0.24       567\n",
      "           7       0.26      0.28      0.27       447\n",
      "           8       0.20      0.13      0.16       595\n",
      "           9       0.35      0.26      0.30       408\n",
      "          10       0.36      0.49      0.41       554\n",
      "          11       0.23      0.12      0.16       156\n",
      "          12       0.08      0.01      0.02        90\n",
      "          13       0.24      0.15      0.19       220\n",
      "          14       0.30      0.45      0.36       758\n",
      "          15       0.19      0.23      0.21       690\n",
      "\n",
      "    accuracy                           0.29      6400\n",
      "   macro avg       0.28      0.25      0.26      6400\n",
      "weighted avg       0.29      0.29      0.28      6400\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.28796875"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = train_model(X_train_vectors, X_test_vectors, y_train, y_test)\n",
    "evaluate(X_test_vectors, y_test, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7125     Religion&Philosophy&Ethics\n",
       "15247        Mechanical_engineering\n",
       "1675                     Psychology\n",
       "9919               Natural_sciences\n",
       "15458             Civil_engineering\n",
       "                    ...            \n",
       "10776        Engineering&Technology\n",
       "14332                       Biology\n",
       "5318             Business&Economics\n",
       "5053     Religion&Philosophy&Ethics\n",
       "9542             Business&Economics\n",
       "Name: category, Length: 12800, dtype: object"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
